{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42685df5",
   "metadata": {},
   "source": [
    "# Probabilistic Graphical Models, SS 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4401373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import shutil\n",
    "import gensim\n",
    "import pprint\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.datasets import load_files\n",
    "from nltk.corpus import names, stopwords\n",
    "from gensim.models.phrases import Phraser\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "from gensim.models import Phrases, CoherenceModel, LdaModel, HdpModel\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46077f6d",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff91ac3",
   "metadata": {},
   "source": [
    "### 1.1 Load `20NewsGroup` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc160b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVE_NAME_ORIGINAL = \"original.zip\"\n",
    "ARCHIVE_NAME_MODIFIED = \"modified.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef5a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(archive_name):\n",
    "    target_dir = 'data/'\n",
    "    archive_path = os.path.join(target_dir, archive_name)\n",
    "    if archive_name==\"original.zip\":        \n",
    "        path = os.path.join(target_dir, 'original')\n",
    "    else:\n",
    "        path = os.path.join(target_dir, 'modified')\n",
    "       \n",
    "    print(\"Decompressing %s\" %archive_name)    \n",
    "    with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "        if not os.path.exists(path):\n",
    "            zip_ref.extractall(target_dir)\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "            zip_ref.extractall(target_dir) \n",
    "    \n",
    "    # Load text files with categories as subfolder names\n",
    "    # Decode text files, if encoding=None load_files() returns list of bytes\n",
    "    dataset = load_files(path, load_content=True, encoding='latin1')\n",
    "    data = dataset.data\n",
    "    print(\"Data is decompressed!\") \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f35a0",
   "metadata": {},
   "source": [
    "#### Load '20NewsGroup' dataset in two formats: the original (orig), and a slightly modified (mod) version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c393c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressing original.zip\n",
      "Data is decompressed!\n",
      "Number of original newsgroup documents: 19997\n",
      "\n",
      "Decompressing modified.zip\n",
      "Data is decompressed!\n",
      "Number of modified newsgroup documents: 18397\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "dataset_original = load_dataset(ARCHIVE_NAME_ORIGINAL)\n",
    "print('Number of original newsgroup documents: %d\\n' % len(dataset_original))\n",
    "\n",
    "dataset_modified = load_dataset(ARCHIVE_NAME_MODIFIED)\n",
    "print('Number of modified newsgroup documents: %d' % len(dataset_modified))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995236a",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ccd42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the documents' header\n",
    "#def remove_text_header(text):\n",
    " #   header, blankline, content = text.partition('\\n\\n')\n",
    "  #  return content\n",
    "\n",
    "#remove the documents' footer/signature\n",
    "#def remove_text_footer(text):\n",
    " #   text = text.replace('-', '')        \n",
    "  #  content, blankline, footer = text.rpartition('\\n\\n')\n",
    "   # return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6f982a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):  \n",
    "    min_tokens = 25\n",
    "    \n",
    "    # remove document with fewer than 25 tokens\n",
    "    dataset = [paper for paper in dataset if len(paper.split())>min_tokens]\n",
    "    \n",
    "    # remove emails\n",
    "    dataset = [re.sub('\\S*@\\S*\\s?', '', paper) for paper in dataset] \n",
    "        \n",
    "    # convert to lowercase and split document into tokens\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        # remove punctuation, numbers and tokens that contain numbers\n",
    "        tokenizer = RegexpTokenizer(r'[a-zA-Z]{3,}') # r'\\w+'\n",
    "        dataset[i] = dataset[i].lower()\n",
    "        dataset[i] = tokenizer.tokenize(dataset[i]) \n",
    "    \n",
    "    #remove numbers but not words that contains numbers\n",
    "    #data = [[token for token in paper if not token.isnumeric()] for paper in dataset]\n",
    "\n",
    "    # remove tokens with less than three-character string\n",
    "    #dataset = [[token for token in paper if len(token)>2] for paper in dataset]\n",
    "    \n",
    "    # remove stop words\n",
    "    stop_words.extend(['from','subject','re','edu','use','cmu'])\n",
    "    dataset = [[token for token in paper if token not in stop_words] for paper in dataset]\n",
    "    \n",
    "    # add bigrams to docs (only ones that appear 20 times or more, \n",
    "    # with the score of the phrase greater than threshold)\n",
    "    bigram = Phrases(dataset, min_count=20, scoring='npmi', threshold=0.8)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    dataset = [bigram_mod[paper] for paper in dataset]\n",
    "    \n",
    "    #map pos_tag to first character lemmatize() accepts\n",
    "    #tags = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "    #tags(nltk.pos_tag([token])[0][1][0])\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize with corresponding pos_tag: NOUN='n', VERB='v'\n",
    "    dataset = [[lemmatizer.lemmatize(token, 'v') for token in paper] for paper in dataset]\n",
    "    dataset = [[lemmatizer.lemmatize(token, 'n') for token in paper] for paper in dataset]\n",
    "        \n",
    "    # create vocabulary\n",
    "    dictionary = Dictionary(dataset)\n",
    "\n",
    "    # filter out words that occur less than 50 documents, or more than 80% of the documents.\n",
    "    dictionary.filter_extremes(no_below=50, no_above=0.8)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in dataset]\n",
    "\n",
    "    print('Number of unique tokens: %d' % len(dictionary))\n",
    "    print('Number of documents - corpus size: %d' % len(corpus))\n",
    "    \n",
    "        \n",
    "    return dataset, corpus, dictionary   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc9072",
   "metadata": {},
   "source": [
    "#### Preprocess '20NewsGroup' dataset in two formats: the original (orig), and a slightly modified (mod) version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79282985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 19995/19995 [00:01<00:00, 11674.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 4969\n",
      "Number of documents - corpus size: 19995\n",
      "Number of original newsgroup documents: 19995\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 18395/18395 [00:01<00:00, 11426.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 4695\n",
      "Number of documents - corpus size: 18395\n",
      "Number of modified newsgroup documents: 18395\n"
     ]
    }
   ],
   "source": [
    "papers_orig, corpus_orig, dictionary_orig = preprocess(dataset_original)\n",
    "print('Number of original newsgroup documents: %d\\n' % len(papers_orig))\n",
    "    \n",
    "papers_mod, corpus_mod, dictionary_mod = preprocess(dataset_modified)\n",
    "print('Number of modified newsgroup documents: %d' % len(papers_mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304703df",
   "metadata": {},
   "source": [
    "## 2. Latent Dirichlet Allocation (LDA, parametric) topic model using `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663e54d",
   "metadata": {},
   "source": [
    "#### Set training parameters and run the LDA training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe609f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tm1(data, k, corpus, dictionary):\n",
    "    \n",
    "    # Set training parameters.\n",
    "    num_topics = k\n",
    "    chunksize = 100\n",
    "    passes = 20\n",
    "    iterations = 1000\n",
    "    random_state = 0\n",
    "    alpha='auto'\n",
    "    eta='auto'\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make an index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "    \n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        random_state=random_state,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6a67c",
   "metadata": {},
   "source": [
    "## 3. Hierarchical Dirichlet Process (HDP, non-parametric) topic model using `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539aa28",
   "metadata": {},
   "source": [
    "#### Set training parameters and run the HDP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c9d24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tm2(corpus, dictionary):\n",
    "    \n",
    "    # Set training parameters.\n",
    "    chunksize = 100\n",
    "    K = 10 # prior - initial number of topics\n",
    "    T = 50 # max number of topics\n",
    "    alpha=1 \n",
    "    gamma=1 \n",
    "    eta=0.01\n",
    "\n",
    "    # Make an index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "    \n",
    "    model = HdpModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize = chunksize,\n",
    "        K=K,\n",
    "        T=T,\n",
    "        alpha=alpha, \n",
    "        gamma=gamma, \n",
    "        eta=eta\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bff0d1",
   "metadata": {},
   "source": [
    "## 4. Training LDA and HDP models on both versions of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d090409",
   "metadata": {},
   "source": [
    "### 4.1 LDA Training for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b6f620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - ['apple', 'video', 'monitor', 'nasa', 'board', 'cable', 'tech', 'ship', 'multiple', 'cpu']\n",
      "1 - ['comp_sys', 'hardware', 'mac', 'display', 'earth', 'power', 'button', 'fully', 'frequency', 'supply']\n",
      "2 - ['misc', 'alt', 'talk', 'xref', 'cantaloupe_srv', 'religion', 'atheism', 'abortion', 'talk_politics', 'argument']\n",
      "3 - ['science', 'value', 'moral', 'objective', 'morality', 'mean', 'say', 'theory', 'study', 'begin']\n",
      "4 - ['may', 'also', 'question', 'read', 'list', 'post', 'book', 'name', 'group', 'follow']\n",
      "5 - ['car', 'president', 'pay', 'pit', 'rec_autos', 'company', 'period', 'business', 'society', 'tax']\n",
      "6 - ['rec', 'motorcycle', 'bike', 'unless', 'dod', 'match', 'jet', 'mph', 'crash', 'filter']\n",
      "7 - ['would', 'one', 'think', 'say', 'get', 'make', 'know', 'people', 'like', 'time']\n",
      "8 - ['game', 'play', 'rec_sport', 'team', 'hockey', 'fan', 'win', 'george', 'season', 'mile']\n",
      "9 - ['comp', 'window', 'type', 'version', 'program', 'text', 'mit', 'copy', 'widget', 'wisc']\n",
      "10 - ['american', 'player', 'washington', 'sex', 'year', 'men', 'trade', 'series', 'goal', 'activity']\n",
      "11 - ['gun', 'government', 'law', 'talk_politics', 'right', 'control', 'source', 'state', 'fire', 'myers']\n",
      "12 - ['soc_culture', 'jew', 'talk_politics', 'jewish', 'turkish', 'kill', 'israel', 'mideast', 'child', 'war']\n",
      "13 - ['file', 'program', 'use', 'information', 'system', 'key', 'available', 'software', 'number', 'security']\n",
      "14 - ['god', 'christian', 'religion', 'rutgers', 'soc', 'believe', 'server', 'approve', 'exist', 'rutgers_igor']\n",
      "15 - ['ohio_state', 'zaphod_mps', 'news', 'university', 'sei_cis', 'magnesium_club', 'distribution', 'andrew', 'apr_gmt', 'utexas']\n",
      "16 - ['news', 'com', 'write', 'net', 'apr_gmt', 'reference', 'article', 'ans_net', 'howland_reston', 'sender']\n",
      "17 - ['sci', 'space', 'baseball', 'engineer', 'electronics', 'bob', 'canada', 'astro', 'mission', 'high']\n",
      "18 - ['system', 'drive', 'problem', 'run', 'work', 'ibm', 'use', 'help', 'do', 'get']\n",
      "19 - ['graphic', 'forsale', 'card', 'computer', 'data', 'machine', 'sale', 'sell', 'local', 'misc']\n"
     ]
    }
   ],
   "source": [
    "tm1_orig = tm1(papers_orig, 20, corpus_orig, dictionary_orig)\n",
    "\n",
    "ldatopics_orig = [[term for term, wt in tm1_orig.show_topic(n, topn=10)] for n in range(0, tm1_orig.num_topics)]\n",
    "for i, topic in enumerate(ldatopics_orig):\n",
    "        print(i, \"-\", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c56e72",
   "metadata": {},
   "source": [
    "### 4.2 LDA Training for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54d76438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - ['step', 'square', 'suddenly', 'subscribe', 'sub', 'stream', 'stone', 'stockholm', 'tcp', 'specialize']\n",
      "1 - ['car', 'rec_autos', 'washington', 'institute', 'design', 'privacy', 'technology', 'rec_motorcycles', 'bike', 'light']\n",
      "2 - ['key', 'crypt', 'anonymous', 'value', 'clipper', 'chip', 'abortion', 'encryption', 'morality', 'moral']\n",
      "3 - ['comp_sys', 'hardware', 'drive', 'ibm', 'card', 'disk', 'scsi', 'driver', 'board', 'bus']\n",
      "4 - ['soc_culture', 'talk_politics', 'kill', 'israel', 'jewish', 'mideast', 'month', 'woman', 'attack', 'death']\n",
      "5 - ['new', 'computer', 'mail', 'apr', 'andrew', 'post', 'usa', 'forsale', 'work', 'machine']\n",
      "6 - ['use', 'system', 'graphic', 'program', 'information', 'available', 'include', 'data', 'number', 'software']\n",
      "7 - ['group', 'david', 'access', 'president', 'control', 'three', 'american', 'water', 'plan', 'member']\n",
      "8 - ['state', 'inc', 'etc', 'company', 'fire', 'med', 'several', 'expect', 'remember', 'almost']\n",
      "9 - ['misc', 'alt', 'talk', 'xref', 'cantaloupe_srv', 'religion', 'atheism', 'theory', 'science', 'origin']\n",
      "10 - ['time', 'one', 'first', 'also', 'two', 'may', 'year', 'tell', 'many', 'come']\n",
      "11 - ['ohio_state', 'sci', 'zaphod_mps', 'sei_cis', 'space', 'news', 'magnesium_club', 'usenet', 'gatech', 'rochester_udel']\n",
      "12 - ['god', 'believe', 'bible', 'word', 'paul', 'say', 'evidence', 'claim', 'mean', 'mouse']\n",
      "13 - ['list', 'mac', 'bite', 'security', 'apple', 'game', 'video', 'win', 'display', 'monitor']\n",
      "14 - ['christian', 'religion', 'rutgers', 'soc', 'approve', 'church', 'christ', 'rutgers_igor', 'love', 'may']\n",
      "15 - ['government', 'child', 'men', 'talk_politics', 'legal', 'politics', 'sex', 'aid', 'gay', 'people']\n",
      "16 - ['would', 'say', 'one', 'get', 'think', 'know', 'like', 'make', 'people', 'see']\n",
      "17 - ['news', 'com', 'apr_gmt', 'net', 'write', 'reference', 'ans_net', 'howland_reston', 'article', 'sender']\n",
      "18 - ['comp', 'window', 'file', 'do', 'unix', 'run', 'color', 'program', 'max', 'copy']\n",
      "19 - ['right', 'gun', 'people', 'law', 'state', 'live', 'war', 'house', 'today', 'weapon']\n"
     ]
    }
   ],
   "source": [
    "tm1_mod = tm1(papers_mod, 20, corpus_mod, dictionary_mod)\n",
    "\n",
    "ldatopics_mod = [[term for term, wt in tm1_mod.show_topic(n, topn=10)] for n in range(0, tm1_mod.num_topics)]\n",
    "for i, topic in enumerate(ldatopics_mod):\n",
    "        print(i, \"-\", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c04bb1",
   "metadata": {},
   "source": [
    "### 4.3 HDP Training for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b61b1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - ['news', 'write', 'one', 'would', 'say', 'article', 'apr_gmt', 'get', 'com', 'reference']\n",
      "1 - ['news', 'window', 'comp', 'apr_gmt', 'com', 'ohio_state', 'net', 'write', 'ans_net', 'howland_reston']\n",
      "2 - ['game', 'hockey', 'news', 'pit', 'rec_sport', 'det', 'play', 'bos', 'period', 'win']\n",
      "3 - ['news', 'apr_gmt', 'ohio_state', 'net', 'com', 'reference', 'write', 'university', 'howland_reston', 'ans_net']\n",
      "4 - ['max', 'air', 'cliff', 'window', 'part', 'apr_gmt', 'news', 'evolution', 'tue', 'atom']\n",
      "5 - ['news', 'order', 'howland_reston', 'like', 'namely', 'fun', 'invoke', 'gateway', 'lemieux', 'customer']\n",
      "6 - ['news', 'apr_gmt', 'graphic', 'sender', 'howland_reston', 'ohio_state', 'com', 'net', 'comp', 'kevin']\n",
      "7 - ['key', 'chip', 'weird', 'discredit', 'technically', 'apps', 'sure', 'bite', 'make', 'govern']\n",
      "8 - ['indeed', 'agenda', 'van', 'netters', 'xref', 'ogicse', 'write', 'design', 'near', 'enterprise']\n",
      "9 - ['explosion', 'regularly', 'teach', 'nothing', 'tool', 'militia', 'device', 'harvard', 'ksand', 'utexas']\n",
      "10 - ['davidian', 'engineer', 'sudden', 'dump', 'ask', 'ussr', 'misc', 'mere', 'grow', 'readily']\n",
      "11 - ['zaphod_mps', 'numerous', 'underground', 'john', 'manufacturer', 'obvious', 'uunet', 'vital', 'max', 'aclu']\n",
      "12 - ['apple', 'experience', 'pseudo', 'use', 'mideast', 'consumer', 'valve', 'yet', 'depend', 'era']\n",
      "13 - ['russell', 'intellectual', 'terminal', 'intelligent', 'compose', 'sci', 'pin', 'interpretation', 'sugar', 'mario']\n",
      "14 - ['isa', 'orst', 'azeri', 'defeat', 'pittsburgh', 'combat', 'bank', 'ludicrous', 'chapel_hill', 'enforcement']\n",
      "15 - ['man', 'pop', 'theme', 'atom', 'news', 'dsp', 'davidian', 'new', 'utcsri', 'orientation']\n",
      "16 - ['netcom', 'vary', 'wow', 'environmental', 'curve', 'rushdie', 'specify', 'april', 'topic', 'fan']\n",
      "17 - ['period', 'compact', 'math', 'break', 'accelerate', 'shot', 'unlike', 'bosnians', 'project', 'retain']\n",
      "18 - ['hypothesis', 'lamb', 'mathematics', 'bbn', 'remark', 'discharge', 'tor', 'amongst', 'hold', 'particularly']\n",
      "19 - ['ussr', 'thursday', 'upgrade', 'fact', 'laptop', 'relevant', 'terminology', 'see', 'corner', 'shout']\n"
     ]
    }
   ],
   "source": [
    "tm2_orig = tm2(corpus_orig, dictionary_orig)\n",
    "hdptopics_orig = [[term for term, wt in tm2_orig.show_topic(n, topn=10)] for n in range(tm2_orig.m_T)]\n",
    "\n",
    "for i, topic in enumerate(hdptopics_orig[:20]):\n",
    "        print(i, \"-\", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fb5e1",
   "metadata": {},
   "source": [
    "### 4.4 HDP Training for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d59f42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - ['write', 'say', 'news', 'one', 'would', 'people', 'article', 'misc', 'think', 'apr_gmt']\n",
      "1 - ['news', 'window', 'comp', 'apr_gmt', 'com', 'write', 'get', 'ohio_state', 'net', 'one']\n",
      "2 - ['soc_culture', 'armenian', 'turkish', 'talk_politics', 'soviet', 'people', 'muslim', 'armenia', 'greek', 'mideast']\n",
      "3 - ['byte', 'bit', 'news', 'one', 'inc', 'sci', 'com', 'apr_gmt', 'ohio_state', 'push']\n",
      "4 - ['news', 'ohio_state', 'apr_gmt', 'com', 'sender', 'howland_reston', 'ans_net', 'reference', 'zaphod_mps', 'forsale']\n",
      "5 - ['news', 'apr_gmt', 'ohio_state', 'net', 'ans_net', 'com', 'howland_reston', 'crabapple', 'university', 'sender']\n",
      "6 - ['talk', 'fact', 'evolution', 'news', 'gravity', 'model', 'theory', 'article', 'latin', 'get']\n",
      "7 - ['news', 'sender', 'write', 'apr_gmt', 'distribution', 'seize', 'proponent', 'henry', 'com', 'advance']\n",
      "8 - ['com', 'reference', 'news', 'resurrection', 'apr_gmt', 'sexual', 'bottom', 'ohio_state', 'deal', 'want']\n",
      "9 - ['value', 'david', 'atheist', 'acid', 'patrick', 'input', 'french', 'laser', 'worse', 'would']\n",
      "10 - ['key', 'hughes', 'mac', 'handy', 'colorado', 'law', 'string', 'routinely', 'radical', 'animal']\n",
      "11 - ['max', 'air', 'cliff', 'university', 'part', 'ohio_state', 'specialist', 'news', 'mere', 'wrong']\n",
      "12 - ['bone', 'extra', 'contend', 'scsi', 'whatever', 'zaphod_mps', 'marry', 'admittedly', 'cry', 'export_lcs']\n",
      "13 - ['florida', 'access', 'stretch', 'bobby', 'presentation', 'sale', 'ncar', 'bd', 'election', 'yeast']\n",
      "14 - ['key', 'controller', 'condition', 'mi', 'origin', 'apostle', 'reference', 'good', 'church', 'prove']\n",
      "15 - ['key', 'battery', 'target', 'antenna', 'grip', 'gate', 'every', 'cult', 'ussr', 'sit']\n",
      "16 - ['diet', 'arrest', 'adequate', 'theory', 'essential', 'borrow', 'inspire', 'accidentally', 'wright', 'package']\n",
      "17 - ['bad', 'remember', 'fundamentalist', 'freeman', 'currently', 'corner', 'concert', 'dog', 'dependent', 'newspaper']\n",
      "18 - ['modify', 'immediate', 'ibm', 'universe', 'sometimes', 'tail', 'louisiana', 'tissue', 'yugoslavia', 'scsi']\n",
      "19 - ['air', 'dad', 'max', 'restrict', 'onto', 'deliver', 'strong', 'soft', 'instal', 'extreme']\n"
     ]
    }
   ],
   "source": [
    "tm2_mod = tm2(corpus_mod, dictionary_mod)\n",
    "hdptopics_mod = [[term for term, wt in tm2_mod.show_topic(n, topn=10)] for n in range(tm2_mod.m_T)]\n",
    "\n",
    "for i, topic in enumerate(hdptopics_mod[:20]):\n",
    "        print(i, \"-\", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab91ed",
   "metadata": {},
   "source": [
    "## 5. Perplexity and coherence scores analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64207fc",
   "metadata": {},
   "source": [
    "### 5.1 LDA Perplexity and Coherence for all the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70a01ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_lda(papers, corpus, dictionary, model):\n",
    "    perp_score = model.log_perplexity(corpus)\n",
    "    \n",
    "    coherence_model = CoherenceModel(\n",
    "        model=model, \n",
    "        texts=papers, \n",
    "        dictionary=dictionary, \n",
    "        coherence='c_v'\n",
    "    ) \n",
    "    \n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    \n",
    "    return perp_score, coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f943a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bar_graph(coherences, models):\n",
    "    \"\"\"\n",
    "    Function to plot bar graph.\n",
    "    \n",
    "    coherences: list of coherence values\n",
    "    models: model names to mark bars.\n",
    "    \"\"\"\n",
    "    n = len(coherences)\n",
    "    x = np.arange(n)\n",
    "    plt.bar(x, coherences, width=0.5, tick_label=models, align='center')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Coherence Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7913c1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA_orig: Perplexity = -10.421584344381516, Coherence = 0.5114434418314101 \n",
      "\n",
      "LDA_mod: Perplexity = -10.757440247176007, Coherence = 0.48723528818102607 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "perp_lda_orig, coher_lda_orig = calculate_metrics_lda(papers_orig, corpus_orig, dictionary_orig, tm1_orig)\n",
    "print(\"LDA_orig: Perplexity = {}, Coherence = {} \\n\".format(str(perp_lda_orig), str(coher_lda_orig)))\n",
    "perp_lda_mod, coher_lda_mod = calculate_metrics_lda(papers_mod, corpus_mod, dictionary_mod, tm1_mod)\n",
    "print(\"LDA_mod: Perplexity = {}, Coherence = {} \\n\".format(str(perp_lda_mod), str(coher_lda_mod)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87ae576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDP_orig: Coherence = 0.43294666909715784 \n",
      "\n",
      "HDP_mod: Coherence = 0.45432896328257 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "coher_hdp_orig = CoherenceModel(\n",
    "    topics=hdptopics_orig[:20], \n",
    "    texts=papers_orig, \n",
    "    dictionary=dictionary_orig).get_coherence()\n",
    "\n",
    "print(\"HDP_orig: Coherence = {} \\n\".format(str(coher_hdp_orig)))\n",
    "\n",
    "coher_hdp_mod = CoherenceModel(\n",
    "    topics=hdptopics_mod[:20], \n",
    "    texts=papers_mod, \n",
    "    dictionary=dictionary_mod).get_coherence()\n",
    "\n",
    "print(\"HDP_mod: Coherence = {} \\n\".format(str(coher_hdp_mod)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae512023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "coherences = [coher_lda_orig, coher_lda_mod, coher_hdp_orig, coher_hdp_mod]\n",
    "model_names = ['LDA_orig', 'LDA_mod', 'HDP_orig', 'HDP_mod']\n",
    "\n",
    "evaluate_bar_graph(coherences, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d74eff",
   "metadata": {},
   "source": [
    "### 5.2 What are the differences in topics for the two dataset versions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a0e83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ec50fd6",
   "metadata": {},
   "source": [
    "### 5.3 Effect of data size and number of topics on perplexity and coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc70d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def effectOfDataAndK_LDA(datasets, datasetNames, K):\n",
    "    print(\"K: \", K)\n",
    "    print(\"total Tests: {} datasets * {} values of k = {}\".format(str(len(datasets)), str(len(K)), \n",
    "                                                                  str(len(datasets) * len(K))))\n",
    "    perplexityScores = {} \n",
    "    coherenceScores = {}\n",
    "    for datasetName, papers in zip(datasetNames, datasets):\n",
    "        print(\"running for dataset \", datasetName)\n",
    "        data_prep, corpus, dictionary = preprocess(papers)\n",
    "        \n",
    "        perplexityForK = {}\n",
    "        coherenceForK = {}\n",
    "        for k in K:\n",
    "            model = tm1(data_prep, k, corpus, dictionary)\n",
    "            perp_score, coherence_lda = calculate_metrics_lda(data_prep, corpus, dictionary, model)\n",
    "            \n",
    "            print(\"For {} and k = {}: perplexity = {}, coherence = {}\".format(datasetName, str(k), \n",
    "                                                                              str(perp_score), str(coherence_lda)))\n",
    "            perplexityForK[str(k)] = perp_score\n",
    "            coherenceForK[str(k)] = coherence_lda\n",
    "            \n",
    "        perplexityScores[datasetName] = perplexityForK\n",
    "        coherenceScores[datasetName] = coherenceForK\n",
    "        \n",
    "    return perplexityScores, coherenceScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67337138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perplexity(score, data_name):\n",
    "    \n",
    "    #plotting perplexity\n",
    "    %matplotlib inline\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "    K = [i*3 for i in range(1, 11)]\n",
    "    fig.tight_layout()\n",
    "\n",
    "    #graph when N is same for all k\n",
    "    for i in range(len(data_name)):\n",
    "        perp_scoreDict = score[data_name[i]]\n",
    "        perp_scores = []\n",
    "        for numTopics in K:\n",
    "            perp_scores.append(perp_scoreDict[str(numTopics)])\n",
    "        \n",
    "        axes[i].set_xticks(K)\n",
    "        axes[i].plot(K, perp_scores)\n",
    "        axes[i].set_xlabel(\"number of topics (k)\")\n",
    "        axes[i].set_ylabel(\"log_Perplexity\")\n",
    "        axes[i].set_title(data_name[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e199b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coherence(score, data_name):\n",
    "    \n",
    "    #plotting perplexity\n",
    "    %matplotlib inline\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "    K = [i*3 for i in range(1, 11)]\n",
    "    fig.tight_layout()\n",
    "\n",
    "    #graph when N is same for all k\n",
    "    for i in range(len(data_name)):\n",
    "        coher_scoreDict = score[data_name[i]]\n",
    "        coher_scores = []\n",
    "        for numTopics in K:\n",
    "            coher_scores.append(coher_scoreDict[str(numTopics)])\n",
    "        \n",
    "        axes[i].set_xticks(K)\n",
    "        axes[i].plot(K, coher_scores)\n",
    "        axes[i].set_xlabel(\"number of topics (k)\")\n",
    "        axes[i].set_ylabel(\"Coherence\")\n",
    "        axes[i].set_title(data_name[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c4619",
   "metadata": {},
   "source": [
    "#### 5.3.1 Making three datasets with 11998 (60%), 15998 (80%), and 19997(100%) documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff718d9b",
   "metadata": {},
   "source": [
    "###### Randomly sampling papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5a8e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(data, numSamples):\n",
    "    randomSamples = random.sample(data, numSamples)\n",
    "    return randomSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4d2d3",
   "metadata": {},
   "source": [
    "##### 5.3.1.1 LDA for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1987e621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressing original.zip\n",
      "Data is decompressed!\n"
     ]
    }
   ],
   "source": [
    "data_original = load_dataset(ARCHIVE_NAME_ORIGINAL)\n",
    "\n",
    "#sample the data\n",
    "papers_orig_60 = sample(data_original, 11998)\n",
    "papers_orig_80 = sample(data_original, 15998)\n",
    "papers_orig_100 = data_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4118176b",
   "metadata": {},
   "source": [
    "#### Checking the effect of change in 'k' and 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c1e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:  [3, 6, 9, 12, 15, 18, 21, 24, 27, 30]\n",
      "total Tests: 3 datasets * 10 values of k = 30\n",
      "running for dataset  papers_orig_60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11997/11997 [00:01<00:00, 8271.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 3623\n",
      "Number of documents - corpus size: 11997\n",
      "For papers_orig_60 and k = 3: perplexity = -7.211666332333588, coherence = 0.6210010844275686\n",
      "For papers_orig_60 and k = 6: perplexity = -7.2439904332563865, coherence = 0.5213020072361709\n",
      "For papers_orig_60 and k = 9: perplexity = -7.249978540113618, coherence = 0.5388117704489039\n",
      "For papers_orig_60 and k = 12: perplexity = -7.3604154127031425, coherence = 0.49838567574888687\n",
      "For papers_orig_60 and k = 15: perplexity = -8.934581837026377, coherence = 0.5162520776763683\n",
      "For papers_orig_60 and k = 18: perplexity = -9.303193624529916, coherence = 0.4735133045568129\n"
     ]
    }
   ],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_paper_orig = [papers_orig_60, papers_orig_80, papers_orig_100]\n",
    "datasetNames_orig = [\"papers_orig_60\", \"papers_orig_80\", \"papers_orig_100\"]\n",
    "ldaperp_orig, ldacoher_orig = effectOfDataAndK_LDA(datasets_paper_orig, datasetNames_orig, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexity(ldaperp_orig, datasetNames_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411efc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(ldacoher_orig, datasetNames_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b0eb1",
   "metadata": {},
   "source": [
    "##### 5.3.1.2 LDA for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42dbd34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressing modified.zip\n",
      "Data is decompressed!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m data_modified \u001b[38;5;241m=\u001b[39m load_dataset(ARCHIVE_NAME_MODIFIED)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#sample the data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m papers_mod_60 \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m(data_modified, \u001b[38;5;241m11998\u001b[39m)\n\u001b[0;32m      5\u001b[0m papers_mod_80 \u001b[38;5;241m=\u001b[39m sample(data_modified, \u001b[38;5;241m15998\u001b[39m)\n\u001b[0;32m      6\u001b[0m papers_mod_100 \u001b[38;5;241m=\u001b[39m data_modified\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "data_modified = load_dataset(ARCHIVE_NAME_MODIFIED)\n",
    "\n",
    "#sample the data\n",
    "papers_mod_60 = sample(data_modified, 11998)\n",
    "papers_mod_80 = sample(data_modified, 15998)\n",
    "papers_mod_100 = data_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b54cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_paper_mod = [papers_mod_60, papers_mod_80, papers_mod_100]\n",
    "datasetNames_mod = [\"papers_mod_60\", \"papers_mod_80\", \"papers_mod_100\"]\n",
    "ldaperp_mod, ldacoher_mod = effectOfDataAndK_LDA(datasets_paper_mod, datasetNames_mod, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50541747",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexity(perpScores_mod, datasetNames_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ac0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(coherScores_mod, datasetNames_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f5919",
   "metadata": {},
   "source": [
    "##### 5.3.1.3 HDP for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def effectOfDataAndK_HDP(datasets, datasetNames, K):\n",
    "    print(\"K: \", K)\n",
    "    print(\"total Tests: {} datasets * {} values of k = {}\".format(str(len(datasets)), str(len(K)), \n",
    "                                                                  str(len(datasets) * len(K))))\n",
    "    coherenceScores = {}\n",
    "    for datasetName, papers in zip(datasetNames, datasets):\n",
    "        print(\"running for dataset \", datasetName)\n",
    "        data_prep, corpus, dictionary = preprocess(papers)\n",
    "        \n",
    "        model = tm2(corpus, dictionary)\n",
    "        hdptopics = [[term for term, wt in model.show_topic(n, topn=10)] for n in range(model.m_T)]\n",
    "        \n",
    "        coherenceForK = {}\n",
    "        for k in K:            \n",
    "            coherence_hdp = CoherenceModel(\n",
    "                topics=hdptopics[:k], \n",
    "                texts=data_prep, \n",
    "                dictionary=dictionary).get_coherence()\n",
    "            \n",
    "            print(\"For {} and k = {}: coherence = {}\".format(datasetName, str(k), str(coherence_hdp)))\n",
    "            coherenceForK[str(k)] = coherence_hdp\n",
    "            \n",
    "        coherenceScores[datasetName] = coherenceForK\n",
    "        \n",
    "    return coherenceScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48842eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_paper_orig = [papers_orig_60, papers_orig_80, papers_orig_100]\n",
    "datasetNames_orig = [\"papers_orig_60\", \"papers_orig_80\", \"papers_orig_100\"]\n",
    "hdpcoher_orig = effectOfDataAndK_HDP(datasets_paper_orig, datasetNames_orig, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(hdpcoher_orig, datasetNames_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9d532",
   "metadata": {},
   "source": [
    "##### 5.3.1.4 HDP for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef451a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_paper_mod = [papers_mod_60, papers_mod_80, papers_mod_100]\n",
    "datasetNames_mod = [\"papers_mod_60\", \"papers_mod_80\", \"papers_mod_100\"]\n",
    "hdpcoher_mod = effectOfDataAndK_HDP(datasets_paper_mod, datasetNames_mod, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(hdpcoher_mod, datasetNames_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1a907",
   "metadata": {},
   "source": [
    "#### 5.3.2 Making three datasets with 500 000, 1 500 000, 2 500 000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceBasedCorpus(data, N):\n",
    "    # M = 19997 for data_original\n",
    "    # M = 18397 for data_modified\n",
    "    #M samples will have N/M number of sentences from each sample\n",
    "    #get sentences from the data\n",
    "    \n",
    "    sentences = [sent_tokenize(paper) for paper in data]\n",
    "    n = int(N/len(sentences))\n",
    "    sentenceData = [\"\".join(i[:n]) for i in sentences if len(i) != 0]\n",
    "    \n",
    "    return sentenceData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd67ac",
   "metadata": {},
   "source": [
    "##### 5.3.2.1 LDA for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "sentences_500000_orig = getSentenceBasedCorpus(data_original, 500000)\n",
    "sentences_1500000_orig = getSentenceBasedCorpus(data_original, 1500000)\n",
    "sentences_2500000_orig = getSentenceBasedCorpus(data_original, 2500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f577a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_sentence_orig = [sentences_500000_orig, sentences_1500000_orig, sentences_2500000_orig]\n",
    "datasetNames_sent_orig = [\"sent_orig_500000\", \"sent_orig_1500000\", \"sent_orig_2500000\"]\n",
    "ldaperp_sent_orig, ldacoher_sent_orig = effectOfDataAndK(datasets_sentence_orig, datasetNames_sent_orig, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5105e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexity(ldaperp_sent_orig, datasetNames_sent_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed146f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_coherence(ldacoher_sent_orig, datasetNames_sent_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3378df",
   "metadata": {},
   "source": [
    "##### 5.3.2.2 LDA for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41903637",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "sentences_500000_mod = getSentenceBasedCorpus(data_modified, 500000)\n",
    "sentences_1500000_mod = getSentenceBasedCorpus(data_modified, 1500000)\n",
    "sentences_2500000_mod = getSentenceBasedCorpus(data_modified, 2500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_sentence_mod = [sentences_500000_mod, sentences_1500000_mod, sentences_2500000_mod]\n",
    "datasetNames_sent_mod = [\"sent_mod_500000\", \"sent_mod_1500000\", \"sent_mod_2500000\"]\n",
    "ldaperp_sent_mod, ldacoher_sent_mod = effectOfDataAndK(datasets_sentence_mod, datasetNames_sent_mod, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77513987",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexity(ldaperp_sent_mod, datasetNames_sent_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(ldacoher_sent_mod, datasetNames_sent_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924e803",
   "metadata": {},
   "source": [
    "##### 5.3.2.3 HDP for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_sentence_orig = [sentences_500000_orig, sentences_1500000_orig, sentences_2500000_orig]\n",
    "datasetNames_sent_orig = [\"sent_orig_500000\", \"sent_orig_1500000\", \"sent_orig_2500000\"]\n",
    "hdpcoher_sent_orig = effectOfDataAndK_HDP(datasets_sentence_orig, datasetNames_sent_orig, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef180d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(hdpcoher_sent_orig, datasetNames_sent_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a866cb9",
   "metadata": {},
   "source": [
    "##### 5.3.2.4 HDP for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4377438",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_sentence_mod = [sentences_500000_mod, sentences_1500000_mod, sentences_2500000_mod]\n",
    "datasetNames_sent_mod = [\"sent_mod_500000\", \"sent_mod_1500000\", \"sent_mod_2500000\"]\n",
    "hdpcoher_sent_mod = effectOfDataAndK_HDP(datasets_sentence_mod, datasetNames_sent_mod, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc31439",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(hdpcoher_sent_mod, datasetNames_sent_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8ddf8",
   "metadata": {},
   "source": [
    "## 6. Further improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep, corpus, dictionary = preprocess(papers)\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 20\n",
    "chunksize = 100\n",
    "passes = 20\n",
    "iterations = 1000\n",
    "random_state = 0\n",
    "alpha=[0.01, 0.05, 0.1, 0.25, 1.0, 5.0]\n",
    "eta=[0.01, 0.05, 0.1]\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "for a in alpha:\n",
    "    for b in eta:\n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            chunksize=chunksize,\n",
    "            alpha=a,\n",
    "            eta=b,\n",
    "            iterations=iterations,\n",
    "            num_topics=num_topics,\n",
    "            passes=passes,\n",
    "            random_state=random_state,\n",
    "            eval_every=eval_every\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f23fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.01, 0.05, 0.1, 0.25, 1.0, 5.0]\n",
    "eta = [0.01, 0.05, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55eb613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
