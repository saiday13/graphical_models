{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42685df5",
   "metadata": {},
   "source": [
    "# Probabilistic Graphical Models, SS 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import shutil\n",
    "import gensim\n",
    "import pprint\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.datasets import load_files\n",
    "from nltk.corpus import names, stopwords\n",
    "from gensim.models.phrases import Phraser\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "from gensim.models import Phrases, CoherenceModel, LdaModel, HdpModel\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46077f6d",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff91ac3",
   "metadata": {},
   "source": [
    "### 1.1 Load `20NewsGroup` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc160b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVE_NAME_ORIGINAL = \"original.zip\"\n",
    "ARCHIVE_NAME_MODIFIED = \"modified.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(archive_name):\n",
    "    target_dir = 'data/'\n",
    "    archive_path = os.path.join(target_dir, archive_name)\n",
    "    if archive_name==\"original.zip\":        \n",
    "        path = os.path.join(target_dir, 'original')\n",
    "    else:\n",
    "        path = os.path.join(target_dir, 'modified')\n",
    "       \n",
    "    print(\"Decompressing %s\" %archive_name)    \n",
    "    with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "        if not os.path.exists(path):\n",
    "            zip_ref.extractall(target_dir)\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "            zip_ref.extractall(target_dir) \n",
    "    \n",
    "    # Load text files with categories as subfolder names\n",
    "    # Decode text files, if encoding=None load_files() returns list of bytes\n",
    "    dataset = load_files(path, load_content=True, encoding='latin1')\n",
    "    data = dataset.data\n",
    "    print(\"Data is decompressed!\") \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f35a0",
   "metadata": {},
   "source": [
    "#### Load '20NewsGroup' dataset in two formats: the original (orig), and a slightly modified (mod) version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c393c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "dataset_original = load_dataset(ARCHIVE_NAME_ORIGINAL)\n",
    "print('Number of original newsgroup documents: %d\\n' % len(dataset_original))\n",
    "\n",
    "dataset_modified = load_dataset(ARCHIVE_NAME_MODIFIED)\n",
    "print('Number of modified newsgroup documents: %d' % len(dataset_modified))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995236a",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the documents' header\n",
    "#def remove_text_header(text):\n",
    " #   header, blankline, content = text.partition('\\n\\n')\n",
    "  #  return content\n",
    "\n",
    "#remove the documents' footer/signature\n",
    "#def remove_text_footer(text):\n",
    " #   text = text.replace('-', '')        \n",
    "  #  content, blankline, footer = text.rpartition('\\n\\n')\n",
    "   # return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f982a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):  \n",
    "    min_tokens = 25\n",
    "    \n",
    "    # remove document with fewer than 25 tokens\n",
    "    dataset = [paper for paper in dataset if len(paper.split())>min_tokens]\n",
    "    \n",
    "    # remove emails\n",
    "    dataset = [re.sub('\\S*@\\S*\\s?', '', paper) for paper in dataset] \n",
    "        \n",
    "    # convert to lowercase and split document into tokens\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        # remove punctuation, numbers and tokens that contain numbers\n",
    "        tokenizer = RegexpTokenizer(r'[a-zA-Z]{3,}') # r'\\w+'\n",
    "        dataset[i] = dataset[i].lower()\n",
    "        dataset[i] = tokenizer.tokenize(dataset[i]) \n",
    "    \n",
    "    #remove numbers but not words that contains numbers\n",
    "    #data = [[token for token in paper if not token.isnumeric()] for paper in dataset]\n",
    "\n",
    "    # remove tokens with less than three-character string\n",
    "    #dataset = [[token for token in paper if len(token)>2] for paper in dataset]\n",
    "    \n",
    "    # remove stop words\n",
    "    stop_words.extend(['from','subject','re','edu','use','cmu'])\n",
    "    dataset = [[token for token in paper if token not in stop_words] for paper in dataset]\n",
    "    \n",
    "    # add bigrams to docs (only ones that appear 20 times or more, \n",
    "    # with the score of the phrase greater than threshold)\n",
    "    bigram = Phrases(dataset, min_count=20, scoring='npmi', threshold=0.8)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    dataset = [bigram_mod[paper] for paper in dataset]\n",
    "    \n",
    "    #map pos_tag to first character lemmatize() accepts\n",
    "    #tags = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "    #tags(nltk.pos_tag([token])[0][1][0])\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize with corresponding pos_tag: NOUN='n', VERB='v'\n",
    "    dataset = [[lemmatizer.lemmatize(token, 'v') for token in paper] for paper in dataset]\n",
    "    dataset = [[lemmatizer.lemmatize(token, 'n') for token in paper] for paper in dataset]\n",
    "        \n",
    "    # create vocabulary\n",
    "    dictionary = Dictionary(dataset)\n",
    "\n",
    "    # filter out words that occur less than 50 documents, or more than 80% of the documents.\n",
    "    dictionary.filter_extremes(no_below=50, no_above=0.8)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in dataset]\n",
    "\n",
    "    print('Number of unique tokens: %d' % len(dictionary))\n",
    "    print('Number of documents - corpus size: %d' % len(corpus))\n",
    "    \n",
    "        \n",
    "    return dataset, corpus, dictionary   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc9072",
   "metadata": {},
   "source": [
    "#### Preprocess '20NewsGroup' dataset in two formats: the original (orig), and a slightly modified (mod) version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79282985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers_orig, corpus_orig, dictionary_orig = preprocess(dataset_original)\n",
    "print('Number of original newsgroup documents: %d\\n' % len(papers_orig))\n",
    "    \n",
    "papers_mod, corpus_mod, dictionary_mod = preprocess(dataset_modified)\n",
    "print('Number of modified newsgroup documents: %d' % len(papers_mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304703df",
   "metadata": {},
   "source": [
    "## 2. Latent Dirichlet Allocation (LDA, parametric) topic model using `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663e54d",
   "metadata": {},
   "source": [
    "#### Set training parameters and run the LDA training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe609f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tm1(data, k, corpus, dictionary):\n",
    "    \n",
    "    # Set training parameters.\n",
    "    num_topics = k\n",
    "    chunksize = 100\n",
    "    passes = 20\n",
    "    iterations = 1000\n",
    "    random_state = 0\n",
    "    alpha='auto'\n",
    "    eta='auto'\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make an index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "    \n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        random_state=random_state,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6a67c",
   "metadata": {},
   "source": [
    "## 3. Hierarchical Dirichlet Process (HDP, non-parametric) topic model using `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539aa28",
   "metadata": {},
   "source": [
    "#### Set training parameters and run the HDP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tm2(corpus, dictionary):\n",
    "    \n",
    "    # Set training parameters.\n",
    "    chunksize = 100\n",
    "    K = 10 # prior - initial number of topics\n",
    "    T = 50 # max number of topics\n",
    "    alpha=1 \n",
    "    gamma=1 \n",
    "    eta=0.01\n",
    "\n",
    "    # Make an index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "    \n",
    "    model = HdpModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize = chunksize,\n",
    "        K=K,\n",
    "        T=T,\n",
    "        alpha=alpha, \n",
    "        gamma=gamma, \n",
    "        eta=eta\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bff0d1",
   "metadata": {},
   "source": [
    "## 4. Training LDA and HDP models on both versions of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d090409",
   "metadata": {},
   "source": [
    "### 4.1 LDA Training for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm1_orig = tm1(papers_orig, 20, corpus_orig, dictionary_orig)\n",
    "\n",
    "ldatopics_orig = [[term for term, wt in tm1_orig.show_topic(n, topn=10)] for n in range(0, tm1_orig.num_topics)]\n",
    "for i, topic in enumerate(ldatopics_orig):\n",
    "        print(i, \"-\", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c56e72",
   "metadata": {},
   "source": [
    "### 4.2 LDA Training for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d76438",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm1_mod = tm1(papers_mod, 20, corpus_mod, dictionary_mod)\n",
    "\n",
    "ldatopics_mod = [[term for term, wt in tm1_mod.show_topic(n, topn=10)] for n in range(0, tm1_mod.num_topics)]\n",
    "for i, topic in enumerate(ldatopics_mod):\n",
    "        print(i, \"-\", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c04bb1",
   "metadata": {},
   "source": [
    "### 4.3 HDP Training for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b1c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm2_orig = tm2(corpus_orig, dictionary_orig)\n",
    "hdptopics_orig = [[term for term, wt in tm2_orig.show_topic(n, topn=10)] for n in range(tm2_orig.m_T)]\n",
    "\n",
    "for i, topic in enumerate(hdptopics_orig[:20]):\n",
    "        print(i, \"-\", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fb5e1",
   "metadata": {},
   "source": [
    "### 4.4 HDP Training for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm2_mod = tm2(corpus_mod, dictionary_mod)\n",
    "hdptopics_mod = [[term for term, wt in tm2_mod.show_topic(n, topn=10)] for n in range(tm2_mod.m_T)]\n",
    "\n",
    "for i, topic in enumerate(hdptopics_mod[:20]):\n",
    "        print(i, \"-\", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab91ed",
   "metadata": {},
   "source": [
    "## 5. Perplexity and coherence scores analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64207fc",
   "metadata": {},
   "source": [
    "### 5.1 LDA Perplexity and Coherence for all the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a01ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_lda(papers, corpus, dictionary, model):\n",
    "    perp_score = model.log_perplexity(corpus)\n",
    "    \n",
    "    coherence_model = CoherenceModel(\n",
    "        model=model, \n",
    "        texts=papers, \n",
    "        dictionary=dictionary, \n",
    "        coherence='c_v'\n",
    "    ) \n",
    "    \n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    \n",
    "    return perp_score, coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bar_graph(coherences, models):\n",
    "    \"\"\"\n",
    "    Function to plot bar graph.\n",
    "    \n",
    "    coherences: list of coherence values\n",
    "    models: model names to mark bars.\n",
    "    \"\"\"\n",
    "    n = len(coherences)\n",
    "    x = np.arange(n)\n",
    "    plt.bar(x, coherences, width=0.5, tick_label=models, align='center')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Coherence Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "perp_lda_orig, coher_lda_orig = calculate_metrics_lda(papers_orig, corpus_orig, dictionary_orig, tm1_orig)\n",
    "print(\"LDA_orig: Perplexity = {}, Coherence = {} \\n\".format(str(perp_lda_orig), str(coher_lda_orig)))\n",
    "perp_lda_mod, coher_lda_mod = calculate_metrics_lda(papers_mod, corpus_mod, dictionary_mod, tm1_mod)\n",
    "print(\"LDA_mod: Perplexity = {}, Coherence = {} \\n\".format(str(perp_lda_mod), str(coher_lda_mod)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coher_hdp_orig = CoherenceModel(\n",
    "    topics=hdptopics_orig[:20], \n",
    "    texts=papers_orig, \n",
    "    dictionary=dictionary_orig).get_coherence()\n",
    "\n",
    "print(\"HDP_orig: Coherence = {} \\n\".format(str(coher_hdp_orig)))\n",
    "\n",
    "coher_hdp_mod = CoherenceModel(\n",
    "    topics=hdptopics_mod[:20], \n",
    "    texts=papers_mod, \n",
    "    dictionary=dictionary_mod).get_coherence()\n",
    "\n",
    "print(\"HDP_mod: Coherence = {} \\n\".format(str(coher_hdp_mod)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae512023",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherences = [coher_lda_orig, coher_lda_mod, coher_hdp_orig, coher_hdp_mod]\n",
    "model_names = ['LDA_orig', 'LDA_mod', 'HDP_orig', 'HDP_mod']\n",
    "\n",
    "evaluate_bar_graph(coherences, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d74eff",
   "metadata": {},
   "source": [
    "### 5.2 What are the differences in topics for the two dataset versions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a0e83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ec50fd6",
   "metadata": {},
   "source": [
    "### 5.3 Effect of data size and number of topics on perplexity and coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def effectOfDataAndK_LDA(datasets, datasetNames, K):\n",
    "    print(\"K: \", K)\n",
    "    print(\"total Tests: {} datasets * {} values of k = {}\".format(str(len(datasets)), str(len(K)), \n",
    "                                                                  str(len(datasets) * len(K))))\n",
    "    perplexityScores = {} \n",
    "    coherenceScores = {}\n",
    "    for datasetName, papers in zip(datasetNames, datasets):\n",
    "        print(\"running for dataset \", datasetName)\n",
    "        data_prep, corpus, dictionary = preprocess(papers)\n",
    "        \n",
    "        perplexityForK = {}\n",
    "        coherenceForK = {}\n",
    "        for k in K:\n",
    "            model = tm1(data_prep, k, corpus, dictionary)\n",
    "            perp_score, coherence_lda = calculate_metrics_lda(data_prep, corpus, dictionary, model)\n",
    "            \n",
    "            print(\"For {} and k = {}: perplexity = {}, coherence = {}\".format(datasetName, str(k), \n",
    "                                                                              str(perp_score), str(coherence_lda)))\n",
    "            perplexityForK[str(k)] = perp_score\n",
    "            coherenceForK[str(k)] = coherence_lda\n",
    "            \n",
    "        perplexityScores[datasetName] = perplexityForK\n",
    "        coherenceScores[datasetName] = coherenceForK\n",
    "        \n",
    "    return perplexityScores, coherenceScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67337138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perplexity(score, data_name):\n",
    "    \n",
    "    #plotting perplexity\n",
    "    %matplotlib inline\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "    K = [i*3 for i in range(1, 11)]\n",
    "    fig.tight_layout()\n",
    "\n",
    "    #graph when N is same for all k\n",
    "    for i in range(len(data_name)):\n",
    "        perp_scoreDict = score[data_name[i]]\n",
    "        perp_scores = []\n",
    "        for numTopics in K:\n",
    "            perp_scores.append(perp_scoreDict[str(numTopics)])\n",
    "        \n",
    "        axes[i].set_xticks(K)\n",
    "        axes[i].plot(K, perp_scores)\n",
    "        axes[i].set_xlabel(\"number of topics (k)\")\n",
    "        axes[i].set_ylabel(\"log_Perplexity\")\n",
    "        axes[i].set_title(data_name[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e199b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coherence(score, data_name):\n",
    "    \n",
    "    #plotting perplexity\n",
    "    %matplotlib inline\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "    K = [i*3 for i in range(1, 11)]\n",
    "    fig.tight_layout()\n",
    "\n",
    "    #graph when N is same for all k\n",
    "    for i in range(len(data_name)):\n",
    "        coher_scoreDict = score[data_name[i]]\n",
    "        coher_scores = []\n",
    "        for numTopics in K:\n",
    "            coher_scores.append(coher_scoreDict[str(numTopics)])\n",
    "        \n",
    "        axes[i].set_xticks(K)\n",
    "        axes[i].plot(K, coher_scores)\n",
    "        axes[i].set_xlabel(\"number of topics (k)\")\n",
    "        axes[i].set_ylabel(\"Coherence\")\n",
    "        axes[i].set_title(data_name[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c4619",
   "metadata": {},
   "source": [
    "#### 5.3.1 Making three datasets with 11998 (60%), 15998 (80%), and 19997(100%) documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff718d9b",
   "metadata": {},
   "source": [
    "###### Randomly sampling papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(data, numSamples):\n",
    "    randomSamples = random.sample(data, numSamples)\n",
    "    return randomSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4d2d3",
   "metadata": {},
   "source": [
    "##### 5.3.1.1 LDA for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original = load_dataset(ARCHIVE_NAME_ORIGINAL)\n",
    "\n",
    "#sample the data\n",
    "papers_orig_60 = sample(data_original, 11998)\n",
    "papers_orig_80 = sample(data_original, 15998)\n",
    "papers_orig_100 = data_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4118176b",
   "metadata": {},
   "source": [
    "#### Checking the effect of change in 'k' and 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_paper_orig = [papers_orig_60, papers_orig_80, papers_orig_100]\n",
    "datasetNames_orig = [\"papers_orig_60\", \"papers_orig_80\", \"papers_orig_100\"]\n",
    "ldaperp_orig, ldacoher_orig = effectOfDataAndK_LDA(datasets_paper_orig, datasetNames_orig, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexity(ldaperp_orig, datasetNames_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411efc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(ldacoher_orig, datasetNames_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b0eb1",
   "metadata": {},
   "source": [
    "##### 5.3.1.2 LDA for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dbd34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modified = load_dataset(ARCHIVE_NAME_MODIFIED)\n",
    "\n",
    "#sample the data\n",
    "papers_mod_60 = sample(data_modified, 11998)\n",
    "papers_mod_80 = sample(data_modified, 15998)\n",
    "papers_mod_100 = data_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b54cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_paper_mod = [papers_mod_60, papers_mod_80, papers_mod_100]\n",
    "datasetNames_mod = [\"papers_mod_60\", \"papers_mod_80\", \"papers_mod_100\"]\n",
    "ldaperp_mod, ldacoher_mod = effectOfDataAndK_LDA(datasets_paper_mod, datasetNames_mod, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50541747",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexity(perpScores_mod, datasetNames_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ac0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(coherScores_mod, datasetNames_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f5919",
   "metadata": {},
   "source": [
    "##### 5.3.1.3 HDP for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def effectOfDataAndK_HDP(datasets, datasetNames, K):\n",
    "    print(\"K: \", K)\n",
    "    print(\"total Tests: {} datasets * {} values of k = {}\".format(str(len(datasets)), str(len(K)), \n",
    "                                                                  str(len(datasets) * len(K))))\n",
    "    coherenceScores = {}\n",
    "    for datasetName, papers in zip(datasetNames, datasets):\n",
    "        print(\"running for dataset \", datasetName)\n",
    "        data_prep, corpus, dictionary = preprocess(papers)\n",
    "        \n",
    "        model = tm2(corpus, dictionary)\n",
    "        hdptopics = [[term for term, wt in model.show_topic(n, topn=10)] for n in range(model.m_T)]\n",
    "        \n",
    "        coherenceForK = {}\n",
    "        for k in K:            \n",
    "            coherence_hdp = CoherenceModel(\n",
    "                topics=hdptopics[:k], \n",
    "                texts=data_prep, \n",
    "                dictionary=dictionary).get_coherence()\n",
    "            \n",
    "            print(\"For {} and k = {}: coherence = {}\".format(datasetName, str(k), str(coherence_hdp)))\n",
    "            coherenceForK[str(k)] = coherence_hdp\n",
    "            \n",
    "        coherenceScores[datasetName] = coherenceForK\n",
    "        \n",
    "    return coherenceScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48842eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_paper_orig = [papers_orig_60, papers_orig_80, papers_orig_100]\n",
    "datasetNames_orig = [\"papers_orig_60\", \"papers_orig_80\", \"papers_orig_100\"]\n",
    "hdpcoher_orig = effectOfDataAndK_HDP(datasets_paper_orig, datasetNames_orig, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(hdpcoher_orig, datasetNames_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9d532",
   "metadata": {},
   "source": [
    "##### 5.3.1.4 HDP for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef451a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_paper_mod = [papers_mod_60, papers_mod_80, papers_mod_100]\n",
    "datasetNames_mod = [\"papers_mod_60\", \"papers_mod_80\", \"papers_mod_100\"]\n",
    "hdpcoher_mod = effectOfDataAndK_HDP(datasets_paper_mod, datasetNames_mod, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(hdpcoher_mod, datasetNames_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1a907",
   "metadata": {},
   "source": [
    "#### 5.3.2 Making three datasets with 500 000, 1 500 000, 2 500 000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceBasedCorpus(data, N):\n",
    "    # M = 19997 for data_original\n",
    "    # M = 18397 for data_modified\n",
    "    #M samples will have N/M number of sentences from each sample\n",
    "    #get sentences from the data\n",
    "    \n",
    "    sentences = [sent_tokenize(paper) for paper in data]\n",
    "    n = int(N/len(sentences))\n",
    "    sentenceData = [\"\".join(i[:n]) for i in sentences if len(i) != 0]\n",
    "    \n",
    "    return sentenceData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd67ac",
   "metadata": {},
   "source": [
    "##### 5.3.2.1 LDA for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "sentences_500000_orig = getSentenceBasedCorpus(data_original, 500000)\n",
    "sentences_1500000_orig = getSentenceBasedCorpus(data_original, 1500000)\n",
    "sentences_2500000_orig = getSentenceBasedCorpus(data_original, 2500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f577a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_sentence_orig = [sentences_500000_orig, sentences_1500000_orig, sentences_2500000_orig]\n",
    "datasetNames_sent_orig = [\"sent_orig_500000\", \"sent_orig_1500000\", \"sent_orig_2500000\"]\n",
    "ldaperp_sent_orig, ldacoher_sent_orig = effectOfDataAndK(datasets_sentence_orig, datasetNames_sent_orig, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5105e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexity(ldaperp_sent_orig, datasetNames_sent_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed146f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_coherence(ldacoher_sent_orig, datasetNames_sent_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3378df",
   "metadata": {},
   "source": [
    "##### 5.3.2.2 LDA for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41903637",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "sentences_500000_mod = getSentenceBasedCorpus(data_modified, 500000)\n",
    "sentences_1500000_mod = getSentenceBasedCorpus(data_modified, 1500000)\n",
    "sentences_2500000_mod = getSentenceBasedCorpus(data_modified, 2500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_sentence_mod = [sentences_500000_mod, sentences_1500000_mod, sentences_2500000_mod]\n",
    "datasetNames_sent_mod = [\"sent_mod_500000\", \"sent_mod_1500000\", \"sent_mod_2500000\"]\n",
    "ldaperp_sent_mod, ldacoher_sent_mod = effectOfDataAndK(datasets_sentence_mod, datasetNames_sent_mod, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77513987",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexity(ldaperp_sent_mod, datasetNames_sent_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(ldacoher_sent_mod, datasetNames_sent_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924e803",
   "metadata": {},
   "source": [
    "##### 5.3.2.3 HDP for `papers_orig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_sentence_orig = [sentences_500000_orig, sentences_1500000_orig, sentences_2500000_orig]\n",
    "datasetNames_sent_orig = [\"sent_orig_500000\", \"sent_orig_1500000\", \"sent_orig_2500000\"]\n",
    "hdpcoher_sent_orig = effectOfDataAndK_HDP(datasets_sentence_orig, datasetNames_sent_orig, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef180d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(hdpcoher_sent_orig, datasetNames_sent_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a866cb9",
   "metadata": {},
   "source": [
    "##### 5.3.2.4 HDP for `papers_mod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4377438",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [i*3 for i in range(1, 11)]\n",
    "datasets_sentence_mod = [sentences_500000_mod, sentences_1500000_mod, sentences_2500000_mod]\n",
    "datasetNames_sent_mod = [\"sent_mod_500000\", \"sent_mod_1500000\", \"sent_mod_2500000\"]\n",
    "hdpcoher_sent_mod = effectOfDataAndK_HDP(datasets_sentence_mod, datasetNames_sent_mod, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc31439",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coherence(hdpcoher_sent_mod, datasetNames_sent_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8ddf8",
   "metadata": {},
   "source": [
    "## 6. Further improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep, corpus, dictionary = preprocess(papers)\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 20\n",
    "chunksize = 100\n",
    "passes = 20\n",
    "iterations = 1000\n",
    "random_state = 0\n",
    "alpha=[0.01, 0.05, 0.1, 0.25, 1.0, 5.0]\n",
    "eta=[0.01, 0.05, 0.1]\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "for a in alpha:\n",
    "    for b in eta:\n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            chunksize=chunksize,\n",
    "            alpha=a,\n",
    "            eta=b,\n",
    "            iterations=iterations,\n",
    "            num_topics=num_topics,\n",
    "            passes=passes,\n",
    "            random_state=random_state,\n",
    "            eval_every=eval_every\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f23fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.01, 0.05, 0.1, 0.25, 1.0, 5.0]\n",
    "eta = [0.01, 0.05, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55eb613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
